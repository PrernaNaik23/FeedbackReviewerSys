{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiRsYj2ppsJSitREccdV9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrernaNaik23/FeedbackReviewerSys/blob/main/feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lCrMaNoaNqUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7d33b4-455f-4bcb-b86b-8fbdbdc3e17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "reviews_path = '/content/Product_Reviews_Final.csv'\n",
        "reviews = pd.read_csv(reviews_path)\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].str.lower()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "punctuation_pattern = r'[^\\w\\s]'\n",
        "text = reviews['Reviews_Text']\n",
        "reviews.drop([3851, 3959], inplace=True)\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].apply(lambda x: re.sub(punctuation_pattern, '', x))\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].str.replace('\\d+', '')\n",
        "reviews = reviews[['Reviews_Text','Reviews_Rating']]\n",
        "reviews['Reviews_Text'].dropna()\n",
        "reviews['Reviews_Text'].reset_index(drop=True)\n",
        "reviews = reviews[reviews['Reviews_Rating']!=3]\n",
        "reviews['label'] = reviews['Reviews_Rating'].apply(lambda x: 1 if x>=4 else 0)\n",
        "reviews['Reviews_Rating'].value_counts()\n",
        "def contractions(s):\n",
        "    s = re.sub(r\"won't\", \"will not\", s)\n",
        "    s = re.sub(r\"wouldn't\", \"would not\", s)\n",
        "    s = re.sub(r\"couldn't\", \"could not\", s)\n",
        "    s = re.sub(r\"can't\", \"can not\", s)\n",
        "    s = re.sub(r\"n't\", \" not\", s)\n",
        "    s = re.sub(r\"'re\", \" are\", s)\n",
        "    s = re.sub(r\"'s\", \" is\", s)\n",
        "    s = re.sub(r\"'ll\", \" will\", s)\n",
        "    s = re.sub(r\"'ve\", \" have\", s)\n",
        "    s = re.sub(r\"'m\", \" am\", s)\n",
        "    s = re.sub(r\"'d\", \" would\", s)\n",
        "    return s\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].apply(contractions)\n",
        "lemma = WordNetLemmatizer()\n",
        "reviews['Reviews_Text'] = reviews['Reviews_Text'].apply(lambda x: ' '.join([lemma.lemmatize(word) for word in nltk.word_tokenize(x)]))\n",
        "model = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def polarity_scores_roberta(text):\n",
        "    encoded_text = tokenizer(text, return_tensors='pt')\n",
        "    output = model(**encoded_text)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    scores_dict = {\n",
        "        'roberta_neg' : scores[0],\n",
        "        'roberta_neu' : scores[1],\n",
        "        'roberta_pos' : scores[2]\n",
        "    }\n",
        "    return scores_dict\n",
        "res = {}\n",
        "for i, row in tqdm(reviews.iterrows(), total=len(reviews)):\n",
        "  try:\n",
        "    text = row['Reviews_Text']\n",
        "    roberta_result = polarity_scores_roberta(text)\n",
        "    res[i] = roberta_result\n",
        "  except RuntimeError:\n",
        "    print(f'Broke for id {i}')\n",
        "polarity_df = pd.DataFrame(res).T\n",
        "reviews = reviews.merge(polarity_df, left_index=True, right_index=True)\n",
        "print(reviews.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvaMYwTVAAiN",
        "outputId": "11ae5132-6b94-4ae1-c355-6ffff6f8a523"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4801/4801 [11:51<00:00,  6.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        Reviews_Text  Reviews_Rating  label  \\\n",
            "0  older version kindle ereader allowed download ...               5      1   \n",
            "1                   far good wife happy miss old one               5      1   \n",
            "2  excellent ereader looking for probably bit pri...               4      1   \n",
            "3  old kindle touch last one still texttovoice lo...               4      1   \n",
            "4  kindle light easy hold long time without getti...               5      1   \n",
            "\n",
            "   roberta_neg  roberta_neu  roberta_pos  \n",
            "0     0.339945     0.598606     0.061448  \n",
            "1     0.008976     0.033567     0.957457  \n",
            "2     0.009584     0.084028     0.906389  \n",
            "3     0.088369     0.434062     0.477569  \n",
            "4     0.014795     0.449097     0.536108  \n"
          ]
        }
      ]
    }
  ]
}